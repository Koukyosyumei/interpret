{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\r\n",
    "from interpret import show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add method to generate selector for mock data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def gen_global_selector(ebm):\r\n",
    "    records = []\r\n",
    "\r\n",
    "    for feature_group_index, feature_indexes in enumerate(\r\n",
    "        ebm.feature_groups_\r\n",
    "    ):\r\n",
    "        record = {}\r\n",
    "        record[\"Name\"] = ebm.feature_names[feature_indexes[0]]\r\n",
    "        record[\"Type\"] = ebm.feature_types[feature_indexes[0]]\r\n",
    "        records.append(record)\r\n",
    "\r\n",
    "    columns = [\"Name\", \"Type\"]\r\n",
    "    df = pd.DataFrame.from_records(records, columns=columns)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add Mock Preprocessor Object (EBMPreprocessor not exposed in interpret)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class AwesomePreprocessor:\r\n",
    "    pass\r\n",
    "\r\n",
    "def _get_bin_labels(self, feature_index):\r\n",
    "    min_val = self.col_min_[feature_index]\r\n",
    "    cuts = self.col_bin_edges_[feature_index]\r\n",
    "    max_val = self.col_max_[feature_index]\r\n",
    "    return list(np.concatenate(([min_val], cuts, [max_val])))\r\n",
    "\r\n",
    "def _get_hist_edges(self, feature_index):\r\n",
    "    return list(self.hist_edges_[feature_index])\r\n",
    "\r\n",
    "def _get_hist_counts(self, feature_index):\r\n",
    "    return list(self.hist_counts_[feature_index])\r\n",
    "\r\n",
    "setattr(AwesomePreprocessor, '_get_bin_labels', _get_bin_labels)\r\n",
    "setattr(AwesomePreprocessor, '_get_hist_edges', _get_hist_edges)\r\n",
    "setattr(AwesomePreprocessor, '_get_hist_counts', _get_hist_counts)\r\n",
    "\r\n",
    "preprocessor = AwesomePreprocessor()\r\n",
    "preprocessor.col_bin_edges_ = {}\r\n",
    "preprocessor.col_bin_edges_[0] = np.array([2, 3])\r\n",
    "preprocessor.col_bin_edges_[1] = np.array([20, 25, 40])\r\n",
    "preprocessor.col_min_ = {}\r\n",
    "preprocessor.col_min_[0] = np.float64(1.0)\r\n",
    "preprocessor.col_min_[1] = np.float64(10)\r\n",
    "preprocessor.col_max_ = {}\r\n",
    "preprocessor.col_max_[0] = np.float64(5)\r\n",
    "preprocessor.col_max_[1] = np.float64(55)\r\n",
    "preprocessor.col_types_ = ['continuous', 'continuous']\r\n",
    "preprocessor.hist_edges_ = {}\r\n",
    "preprocessor.hist_edges_[0] = np.array([])\r\n",
    "preprocessor.hist_edges_[1] = np.array([])\r\n",
    "preprocessor.hist_counts_ = {}\r\n",
    "preprocessor.hist_counts_[0] = np.array([])\r\n",
    "preprocessor.hist_counts_[1] = np.array([])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create EBM Classifier with all mock data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "ebm = ExplainableBoostingClassifier()\r\n",
    "\r\n",
    "ebm.additive_terms_ = list()\r\n",
    "ebm.additive_terms_.append(np.array([0, 1.0, -1.0, 2.0]))\r\n",
    "ebm.additive_terms_.append(np.array([0, -2.0, 10, 0, 7]))\r\n",
    "\r\n",
    "ebm.term_standard_deviations_ = list()\r\n",
    "ebm.term_standard_deviations_.append(np.array([0, .2, .1, .3]))\r\n",
    "ebm.term_standard_deviations_.append(np.array([0, .2, .1, .3, .5]))\r\n",
    "\r\n",
    "ebm.classes_ = np.array([0, 1])\r\n",
    "\r\n",
    "ebm.feature_names = ['feature_1', 'feature_2']\r\n",
    "ebm.feature_types = ['continuous', 'continuous']\r\n",
    "ebm.feature_groups_ = [[0], [1]]\r\n",
    "ebm.feature_importances_ = [np.float64(.45), np.float64(.63)]\r\n",
    "\r\n",
    "ebm.preprocessor_ = preprocessor\r\n",
    "ebm.global_selector = gen_global_selector(ebm)\r\n",
    "\r\n",
    "ebm.has_fitted_ = True\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Does show() work with mocked EBM Classifier?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "ebm_global = ebm.explain_global(name='EBM')\r\n",
    "show(ebm_global)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7357/2924976688192/ -->\n",
       "<iframe src=\"http://127.0.0.1:7357/2924976688192/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Declare `_VisualizationModelWrap`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from typing import List\r\n",
    "from interpret.glassbox import ExplainableBoostingClassifier as E\r\n",
    "import inspect\r\n",
    "\r\n",
    "class _VisualizationModelWrap:\r\n",
    "    def __init__(self, inner_model):\r\n",
    "        self._inner_model = inner_model\r\n",
    "\r\n",
    "    @property\r\n",
    "    def num_features(self) -> int:\r\n",
    "        #return self._inner_model.numFeatures\r\n",
    "        return 2\r\n",
    "\r\n",
    "    @property\r\n",
    "    def feature_names(self) -> List[str]:\r\n",
    "         # return self._inner_model.featureNames\r\n",
    "        return ['feature_1', 'feature_2']\r\n",
    "\r\n",
    "    @property\r\n",
    "    def feature_types(self) -> List[str]:\r\n",
    "        # InterpretML has 'categorical', 'ordinal' and 'continuous', but you just have continuous so I think this will\r\n",
    "        # be a list with |feature| 'continuous' strings for our purposes\r\n",
    "        return ['continuous', 'continuous']\r\n",
    "\r\n",
    "    @property\r\n",
    "    def feature_importances(self) -> List[float]:\r\n",
    "        # TODO: use T-EBMs bin counts and bin weights to calculate feature importances\r\n",
    "        return [0.45, 0.63]\r\n",
    "\r\n",
    "    def _gen_global_selector(self):\r\n",
    "        \"\"\" Generates a Pandas DataFrame from used to render the selector section of a visualization that allows the\r\n",
    "            user to choose different visualizations.\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            A Pandas DataFrame used to render the selector section of a visualization that allows the user to choose\r\n",
    "            different visualizations.\r\n",
    "        \"\"\"\r\n",
    "        records = []\r\n",
    "\r\n",
    "        for feature_idx in range(self.num_features):\r\n",
    "            record = {}\r\n",
    "            record[\"Name\"] = self.feature_names[feature_idx]\r\n",
    "            record[\"Type\"] = self.feature_types[feature_idx]\r\n",
    "            records.append(record)\r\n",
    "\r\n",
    "        columns = [\"Name\", \"Type\"]\r\n",
    "        df = pd.DataFrame.from_records(records, columns=columns)\r\n",
    "        return df\r\n",
    "\r\n",
    "    def _get_bounds(self):\r\n",
    "        # calculate minimum and maximum of all the model scores; InterpretML implementation in comments below\r\n",
    "\r\n",
    "        # Obtain min/max for model scores\r\n",
    "#         lower_bound = np.inf\r\n",
    "#         upper_bound = -np.inf\r\n",
    "#         for feature_group_index, _ in enumerate(self.feature_groups_):\r\n",
    "#             errors = self.term_standard_deviations_[feature_group_index]\r\n",
    "#             scores = self.additive_terms_[feature_group_index]\r\n",
    "#\r\n",
    "#             lower_bound = min(lower_bound, np.min(scores - errors))\r\n",
    "#             upper_bound = max(upper_bound, np.max(scores + errors))\r\n",
    "#\r\n",
    "#         bounds = (lower_bound, upper_bound)\r\n",
    "\r\n",
    "        return (-2.2, 10.1)\r\n",
    "\r\n",
    "    def _get_values_for_bin(self, feature_idx) -> List[float]:\r\n",
    "        # self._inner_model.valuesForBin(feature_idx)\r\n",
    "        if feature_idx == 0:\r\n",
    "            return [1.0, -1.0, 2.0]\r\n",
    "        else:\r\n",
    "            return [-2.0, 10, 0, 7]\r\n",
    "\r\n",
    "    def _get_stddev_for_bin(self, feature_idx) -> List[float]:\r\n",
    "        # self._inner_model.stddevForBin(feature_idx)\r\n",
    "        if feature_idx == 0:\r\n",
    "            return [.2, .1, .3]\r\n",
    "        else:\r\n",
    "            return [.2, .1, .3, .5]\r\n",
    "\r\n",
    "    def _get_feature_min(self, feature_idx) -> float:\r\n",
    "        # self._inner_model.featureMins(feature_idx)\r\n",
    "        if feature_idx == 0:\r\n",
    "            return 1.0\r\n",
    "        else:\r\n",
    "            return 10.0\r\n",
    "\r\n",
    "    def _get_feature_max(self, feature_idx) -> float:\r\n",
    "        # self._inner_model.featureMaxes(feature_idx)\r\n",
    "        if feature_idx == 0:\r\n",
    "            return 5.0\r\n",
    "        else:\r\n",
    "            return 55.0\r\n",
    "\r\n",
    "    def _get_bin_thresholds(self, feature_idx) -> List[float]:\r\n",
    "        # self._inner_model.binThresholds(feature_idx)\r\n",
    "        if feature_idx == 0:\r\n",
    "            return [2.0, 3.0]\r\n",
    "        else:\r\n",
    "            return [20.0, 25.0, 40.0]\r\n",
    "\r\n",
    "    def _get_bin_labels(self, feature_idx) -> List[float]:\r\n",
    "        feature_type = self.feature_types[feature_idx]\r\n",
    "        if feature_type == \"continuous\":\r\n",
    "            min_val = self._get_feature_min(feature_idx)\r\n",
    "            cuts = self._get_bin_thresholds(feature_idx)\r\n",
    "            max_val = self._get_feature_max(feature_idx)\r\n",
    "            return list(np.concatenate(([min_val], cuts, [max_val])))\r\n",
    "        else:  # pragma: no cover\r\n",
    "            raise Exception(\"Unknown feature type\")\r\n",
    "\r\n",
    "    def _create_internal_object(self):\r\n",
    "        data_dicts = []\r\n",
    "        feature_list = []\r\n",
    "\r\n",
    "        for feature_idx in range(self.num_features):\r\n",
    "            model_graph = np.array(self._get_values_for_bin(feature_idx))\r\n",
    "            errors = np.array(self._get_stddev_for_bin(feature_idx))\r\n",
    "\r\n",
    "            bin_labels = self._get_bin_labels(feature_idx)\r\n",
    "            scores = list(model_graph)\r\n",
    "            upper_bounds = list(model_graph + errors)\r\n",
    "            lower_bounds = list(model_graph - errors)\r\n",
    "\r\n",
    "            feature_dict = {\r\n",
    "                \"type\": \"univariate\",\r\n",
    "                \"names\": bin_labels,\r\n",
    "                \"scores\": scores,\r\n",
    "                \"scores_range\": self._get_bounds(),\r\n",
    "                \"upper_bounds\": upper_bounds,\r\n",
    "                \"lower_bounds\": lower_bounds,\r\n",
    "            }\r\n",
    "\r\n",
    "            feature_list.append(feature_dict)\r\n",
    "\r\n",
    "            data_dict = {\r\n",
    "                \"type\": \"univariate\",\r\n",
    "                \"names\": bin_labels,\r\n",
    "                \"scores\": model_graph,\r\n",
    "                \"scores_range\": self._get_bounds(),\r\n",
    "                \"upper_bounds\": model_graph + errors,\r\n",
    "                \"lower_bounds\": model_graph - errors\r\n",
    "            }\r\n",
    "\r\n",
    "            if type(self) is _ClassificationWrap:\r\n",
    "                data_dict[\"meta\"] = {\r\n",
    "                    \"label_names\": self.classes\r\n",
    "                }\r\n",
    "\r\n",
    "            data_dicts.append(data_dict)\r\n",
    "\r\n",
    "        overall_dict = {\r\n",
    "            \"type\": \"univariate\",\r\n",
    "            \"names\": self.feature_names,\r\n",
    "            \"scores\": self.feature_importances,\r\n",
    "        }\r\n",
    "\r\n",
    "        internal_obj = {\r\n",
    "            \"overall\": overall_dict,\r\n",
    "            \"specific\": data_dicts,\r\n",
    "            \"mli\": [\r\n",
    "                {\r\n",
    "                    \"explanation_type\": \"ebm_global\",\r\n",
    "                    \"value\": {\"feature_list\": feature_list}\r\n",
    "                }\r\n",
    "            ]\r\n",
    "        }\r\n",
    "\r\n",
    "        return internal_obj\r\n",
    "\r\n",
    "\r\n",
    "    def explain_global(self, name=None):\r\n",
    "        \"\"\" Provides global explanation for model.\r\n",
    "\r\n",
    "        Args:\r\n",
    "            name: User-defined explanation name.\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            An explanation object,\r\n",
    "            visualizing feature-value pairs as horizontal bar chart.\r\n",
    "        \"\"\"\r\n",
    "        if name is None:\r\n",
    "            name = \"an intelligent default name for the explanation like 'EBM'\"\r\n",
    "\r\n",
    "        return inspect.getmodule(E).EBMExplanation(\r\n",
    "            \"global\",\r\n",
    "            self._create_internal_object(),\r\n",
    "            feature_names=self.feature_names,\r\n",
    "            feature_types=self.feature_types,\r\n",
    "            name=name,\r\n",
    "            selector=self._gen_global_selector()\r\n",
    "        )\r\n",
    "\r\n",
    "class _RegressionWrap(_VisualizationModelWrap):\r\n",
    "    def __init__(self, inner_model):\r\n",
    "        super().__init__(inner_model)\r\n",
    "\r\n",
    "class _ClassificationWrap(_VisualizationModelWrap):\r\n",
    "    def __init__(self, inner_model):\r\n",
    "        super().__init__(inner_model)\r\n",
    "\r\n",
    "    @property\r\n",
    "    def classes(self) -> List[int]:\r\n",
    "         # return self._inner_model.?? -- we might need to implement this\r\n",
    "        return [0, 1]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Does show() work with mocked `_VisualizationModelWrap`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "mocked_viz_wrap = _ClassificationWrap(None)\r\n",
    "ebm_global = mocked_viz_wrap.explain_global(name='EBM')\r\n",
    "show(ebm_global)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7357/2925053644368/ -->\n",
       "<iframe src=\"http://127.0.0.1:7357/2925053644368/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('interpret': conda)"
  },
  "interpreter": {
   "hash": "a7af01c03129762fe64321f262bbde073df4bcc1b9dc61c0aae9860bf41de141"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}